#!/usr/bin/env python3
"""
Quick E2E Test - Pure Streaming (NO Pandas, NO Memory Crashes)
100% CSV streaming with SQLite indexing for large deal data

Time: 2-3 minutes
Tests: Parsing ‚Üí Chunking ‚Üí Pinecone with Financial Data
"""

import os
import sys
from pathlib import Path
from dotenv import load_dotenv

# Setup
sys.path.insert(0, 'src')
sys.path.insert(0, '.')
load_dotenv()

from src.connectors.raw_salesforce_export_connector_pure_streaming import PureStreamingConnector
from src.connectors.pinecone_client import PineconeDocumentClient
from src.config.settings import Settings

# Configuration
EXPORT_DIR = "/Volumes/Jeff_2TB/day_20251202_053804-04f8f8_29771"
MERGED_DEAL_CSV = "/Users/jeffmuscarella/2025_Python/Dropbox/deal_merged_financial_data.csv"
TEST_NAMESPACE = "quick_e2e_pure_streaming_dec2025"
NUM_TEST_DOCS = 10

print("üöÄ Quick E2E Test - Pure Streaming (NO Memory Issues)")
print("=" * 70)
print(f"üìÅ Export: {EXPORT_DIR}")
print(f"üí∞ Deal Data: {Path(MERGED_DEAL_CSV).name} (merged financial)")
print(f"‚ö° Mode: Pure CSV streaming + SQLite indexing")
print(f"üéØ Namespace: {TEST_NAMESPACE}")
print(f"üìÑ Documents: {NUM_TEST_DOCS}")
print()

# Step 1: Initialize connector
print("üìã Step 1: Initializing pure streaming connector...")

try:
    connector = PureStreamingConnector(
        export_root_dir=EXPORT_DIR,
        content_versions_csv=f"{EXPORT_DIR}/content_versions.csv",
        content_documents_csv=f"{EXPORT_DIR}/content_documents.csv",
        content_document_links_csv=f"{EXPORT_DIR}/content_document_links.csv",
        deal_metadata_csv=MERGED_DEAL_CSV,
        deal_mapping_csv="organized_files_to_deal_mapping.csv"
    )
    print("‚úÖ Connector initialized (streaming mode, no memory overhead)")
    
    # Statistics
    print()
    connector.print_export_statistics()
    
except Exception as e:
    print(f"‚ùå Failed: {e}")
    import traceback
    traceback.print_exc()
    sys.exit(1)

# Step 2: Get 10 test documents
print("\nüìã Step 2: Selecting 10 test documents...")

test_docs = []
try:
    for doc_metadata in connector.list_documents_as_metadata(require_deal_association=True):
        if doc_metadata.full_path:
            abs_path = Path(EXPORT_DIR) / doc_metadata.full_path if not Path(doc_metadata.full_path).is_absolute() else Path(doc_metadata.full_path)
            if abs_path.exists():
                test_docs.append(doc_metadata)
                if len(test_docs) >= NUM_TEST_DOCS:
                    break
    
    if not test_docs:
        print("‚ùå No documents found!")
        sys.exit(1)
    
    print(f"‚úÖ Selected {len(test_docs)} documents with financial data:")
    for i, doc in enumerate(test_docs, 1):
        print(f"   {i}. {doc.name}")
        if doc.proposed_amount:
            print(f"      üí∞ ${doc.proposed_amount:,.0f}")

except Exception as e:
    print(f"‚ùå Error: {e}")
    import traceback
    traceback.print_exc()
    sys.exit(1)

# Step 3: Initialize Pinecone and chunker
print("\nüìã Step 3: Initializing Pinecone and chunker...")

try:
    settings = Settings()
    pinecone_client = PineconeDocumentClient(
        api_key=os.getenv("PINECONE_API_KEY"),
        index_name=settings.PINECONE_INDEX_NAME
    )
    
    # Use semantic chunker directly for simpler testing
    from src.chunking.semantic_chunker import SemanticChunker
    chunker = SemanticChunker(
        max_chunk_size=1024,
        overlap_size=128
    )
    print("‚úÖ Pinecone and chunker ready")

except Exception as e:
    print(f"‚ùå Error: {e}")
    import traceback
    traceback.print_exc()
    sys.exit(1)

# Step 4: Process and store
print(f"\nüìã Step 4: Processing {len(test_docs)} documents...")

stored_count = 0
failed = []

try:
    for i, doc in enumerate(test_docs, 1):
        abs_path = Path(EXPORT_DIR) / doc.full_path if not Path(doc.full_path).is_absolute() else Path(doc.full_path)
        
        # If it's a directory, get the first file inside it
        if abs_path.is_dir():
            files = list(abs_path.iterdir())
            if files:
                abs_path = files[0]
            else:
                print(f"   [{i}/{len(test_docs)}] {doc.name} ‚ö†Ô∏è  (empty directory)")
                continue
        
        print(f"   [{i}/{len(test_docs)}] {doc.name}", end=" ")
        
        try:
            with open(abs_path, 'r', encoding='utf-8', errors='ignore') as f:
                content = f.read().strip()
            
            if not content:
                print("‚ö†Ô∏è  (empty)")
                continue
            
            # Convert metadata dataclass to dict for chunker
            from dataclasses import asdict
            metadata_dict = asdict(doc)
            
            # Chunk the content
            chunks = chunker.chunk_document(content, metadata_dict)
            
            if chunks:
                # Store chunks in Pinecone
                stored = pinecone_client.upsert_chunks(
                    chunks=chunks,
                    namespace=TEST_NAMESPACE
                )
                stored = len(chunks) if stored else 0
                if stored > 0:
                    print(f"‚úÖ ({stored} chunks)")
                    stored_count += stored
                else:
                    print("‚ùå (storage failed)")
                    failed.append(doc.name)
            else:
                print("‚ö†Ô∏è  (no chunks)")
        
        except Exception as e:
            print(f"‚ùå ({e})")
            failed.append(doc.name)
    
    print(f"\n‚úÖ Processed {len(test_docs)} documents, stored {stored_count} chunks")
    if failed:
        print(f"   Failed: {len(failed)}")

except Exception as e:
    print(f"‚ùå Error: {e}")
    import traceback
    traceback.print_exc()

# Step 5: Verify
print(f"\nüìã Step 5: Verifying Pinecone storage...")

try:
    dummy_vector = [0.0] * 1024
    query = pinecone_client.pc.Index(settings.PINECONE_INDEX_NAME).query(
        namespace=TEST_NAMESPACE,
        vector=dummy_vector,
        top_k=3,
        include_metadata=True
    )
    
    if query and query.get('matches'):
        print(f"‚úÖ Found {len(query['matches'])} vectors in Pinecone")
        
        print(f"\n   Financial data sample:")
        for j, match in enumerate(query['matches'][:3], 1):
            metadata = match.get('metadata', {})
            print(f"   [{j}] {metadata.get('deal_name', 'N/A')}")
            if metadata.get('proposed_amount'):
                print(f"       üí∞ Proposed: ${float(metadata['proposed_amount']):,.0f}")

except Exception as e:
    print(f"‚ö†Ô∏è  Verification failed: {e}")

# Summary
print("\n" + "=" * 70)
print("‚úÖ PURE STREAMING E2E TEST COMPLETE")
print("=" * 70)
print(f"\nResults:")
print(f"  Documents: {len(test_docs)}")
print(f"  Chunks stored: {stored_count}")
print(f"  Failed: {len(failed)}")
print(f"  Memory mode: Pure streaming (no pandas, no memory crashes)")
print(f"\nüí∞ Financial data successfully integrated into Pinecone!")

