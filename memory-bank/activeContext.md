# Active Context - Business Document Intelligence Platform

**Last Updated**: December 18, 2025  
**Current Focus**: üéØ **Metadata Redaction + Production Cleanup**

---

## üß™ Testing (NOT Production): Redaction Review Artifacts (December 2025)

- **Added testing harness (archived after cleanup)**: `archive/scripts/redaction_harness_local.py`
  - Runs locally against PDFs referenced by a discovery JSON and writes reviewer-friendly artifacts.
  - **Important**: ‚ÄúRuns locally‚Äù means local files + local output folder; **LLM PERSON/ORG redaction still calls OpenAI** when enabled.
- **Important**: Harness output folders are point-in-time. If you fixed redaction logic after a run, you must re-run the harness to see the fix reflected in `redacted.md`.
- **Per-document artifacts** (under `output/redaction_harness_<timestamp>/docs/<doc_id>/`):
  - `original.<ext>` (exact bytes used; e.g. `original.pdf`, `original.docx`)
  - `redacted.md` (markdown body for review; optional metadata header)
  - `redaction.json` (counts, model used, strict-mode validation results, warnings/errors, extraction stats, **`content_parser`**)
- **Run index**:
  - `output/redaction_harness_<timestamp>/REDACTION_REVIEW_INDEX.md` links to each `original.<ext>` + `redacted.md` and includes counts + extracted word count + **`content_parser`**.

### Redaction stage logic (defense-in-depth approach)
- **Location**: `src/redaction/` (see `src/redaction/README.md`)
- **Ordering**: regex PII ‚Üí **deterministic client aliases (FIRST)** ‚Üí (optional) LLM span detection (PERSON + client-matching ORG) ‚Üí conservative cleanup of partial legal-name tails ‚Üí strict-mode validators.
- **Key architectural decision**: Deterministic aliases run **BEFORE** LLM for optimal performance/cost:
  - **80% coverage**: Known aliases (e.g., "AmFam") caught instantly, no LLM cost
  - **No context loss**: LLM receives `client_name` + `client_variants` via parameters even after pre-redaction
  - **Intelligent discovery**: LLM focuses on finding NEW variants (typos, uncommon abbreviations) not in CSV
  - **Fail-safe**: Known aliases always caught, even if LLM service fails
  - **Cost savings**: Pre-redaction reduces LLM token count by ~15-20%
- **Key requirement**: client redaction requires `salesforce_client_id` (otherwise we intentionally cannot know which ORG is "the client").
- **LLM span debugging logs (Dec 2025)**: `src/redaction/llm_span_detector.py` now logs `llm_span_detection_empty_content*` and `llm_span_detection_json_decode_error*` with `finish_reason`, `has_refusal`, `tool_calls_present`, `response_id/model`, and safe `content_sha256_16` (no text).

### Redaction improvements (December 17, 2025)
- **Responses API migration**: Migrated from Chat Completions to Responses API (`client.responses.create()`) with strict JSON Schema output, `reasoning.effort="minimal"`, and default model `gpt-5-mini` (rolling alias).
- **Simplified deterministic layer**: 
  - **Removed** programmatic acronym/abbreviation generation (e.g., "AFI", "BNYM", "AmFam")
  - **Kept** reliable patterns: full names, normalizations, legal suffix stripping, &/and swaps
  - **Rationale**: Cultural/business nicknames (e.g., "AmFam") cannot be derived algorithmically; short acronyms risk false positives (e.g., "MS" matches Morgan Stanley, Microsoft, Mississippi)
- **LLM handles abbreviations**: LLM prompts include general instructions to detect contextual abbreviations/acronyms. This is where abbreviation detection belongs.
- **ORG span filtering**: Uses explicit CSV aliases for matching (human-curated).

### LLM Span Detection Prompt Improvements (December 21, 2025)
- **Vendor context added**: Now passes `vendor_name` from deal metadata to LLM (1 primary vendor, not a list)
  - Example: `"Client: American Family | Vendor: Guidewire (DO NOT detect as ORG)"`
  - Prevents ambiguous cases where client/vendor have similar names
  - Lightweight context (~10-20 tokens) with high impact
- **Few-shot examples added**: 4 concrete scenarios showing client vs vendor distinction (e.g., "AmFam" = client, "Guidewire" = vendor)
- **Goal framing**: Explicit "better to over-detect client mentions than miss them" for redaction safety
- **Structured rules**: Numbered inclusion/exclusion rules (clearer than bullet points)
- **Context signals**: Teaches LLM to recognize client mentions via position (subject vs object), proximity to full name, parenthetical definitions
- **Expected impact**: 10-20% improvement in detecting client abbreviations/nicknames, reduced vendor false positives
- **Token cost**: ~150-200 additional tokens per call (few-shot examples + vendor context worth the cost for accuracy)

### üîÆ Alias Generation System (December 21, 2025)
**Status**: ‚úÖ **PILOT COMPLETE** - LLM-driven alias generation tested on 9 clients

**Approach**: LLM-only with few-shot examples (Option D)
- **Script**: `scripts/generate_aliases_pilot_50.py`
- **Input**: Discovery JSON + Pinecone evidence (500 chunks/client) + Primary vendors from discovery metadata
- **Model**: GPT-5 mini with reasoning + strict JSON schema
- **Process**: LLM analyzes evidence with few-shot examples, returns 3-7 aliases with reasoning, validates verbatim appearance
- **Vendor filtering**: Only **primary vendors from discovery JSON** (1-2 per client), not all Pinecone chunks (prevents 40-179 vendor overflow)

**Results (9-client test)**:
- **Success rate**: 3/9 clients (33%) found aliases
- **Quality**: Zero false positives, all aliases legitimate
- **Examples**: 
  - FIS Global ‚Üí `FIS`, `Fidelity Information Services`, `fisglobal.com`
  - TreeHouse Foods ‚Üí `Treehouse`
- **Limitation**: Conservative (missed obvious ones like "P&G" for Procter & Gamble)

**Key Innovation**: Extract vendors from **discovery JSON only** (documents being processed), not from all Pinecone chunks. This gives clean 1-2 vendor list vs 40-179 vendors that overwhelmed prompts.

**Documentation**: Full details in `src/redaction/ALIAS_GENERATION.md`

**Next Steps**: Run on all 50 clients, evaluate if LLM prompts need strengthening, or consider hybrid approach (deterministic + LLM)

---

## üöÄ Production Redaction Usage (December 2025)

### Pipeline Integration Complete
The redaction stage is **fully integrated** into the document processing pipeline. Enable it with CLI flags:

**Parallel processing with redaction:**
```bash
python process_discovered_documents.py \
  --input discovery.json \
  --namespace production \
  --workers 6 \
  --parser-backend mistral \
  --enable-redaction \
  --client-redaction-csv src/redaction/SF-Cust-Mapping.csv \
  --redaction-model gpt-5-mini-2025-08-07
```

**Serial processing with redaction:**
```bash
python process_discovered_documents.py \
  --input discovery.json \
  --namespace production \
  --enable-redaction \
  --client-redaction-csv src/redaction/SF-Cust-Mapping.csv
```

### Key Flags
| Flag | Description |
|------|-------------|
| `--enable-redaction` | Enables PII redaction stage (runs before chunking) |
| `--client-redaction-csv` | Path to client registry CSV (required for client redaction) |
| `--redaction-model` | OpenAI model for PERSON detection (default: `gpt-5-mini-2025-08-07`) |

### Requirements
- **`salesforce_client_id`** must be present in document metadata (from Salesforce discovery)
- **Client registry CSV** must contain the client ID for client name redaction to apply
- Documents without `salesforce_client_id` will skip client redaction (PII patterns still apply)

### What Gets Redacted
1. **Regex PII** (always): emails ‚Üí `<<EMAIL>>`, phones ‚Üí `<<PHONE>>`, addresses ‚Üí `<<ADDRESS>>`
2. **LLM Spans** (when enabled): person names ‚Üí `<<PERSON>>`, client ORG names ‚Üí `<<CLIENT: ...>>`
3. **Deterministic Client Names**: Remaining client mentions via registry patterns
4. **Metadata Fields** (December 2025): `client_name` ‚Üí `<<CLIENT>>`, client names in `file_name` ‚Üí `<<CLIENT>>`

### Validation
- **Strict mode** enabled by default: fails documents with remaining sensitive content
- Check logs for: `Redaction complete: X replacements (client=Y, email=Z, phone=W, address=V, person=U)`

### Metadata Redaction (December 18, 2025)
When `--enable-redaction` is used, client names are also redacted from Pinecone metadata fields:
- `client_name` ‚Üí `<<CLIENT>>`
- `file_name` ‚Üí client name occurrences replaced with `<<CLIENT>>` (e.g., `"Nasdaq Report.pdf"` ‚Üí `"<<CLIENT>> Report.pdf"`)

**Implementation**: `src/pipeline/parallel_processor.py` - `redact_metadata_fields()` function called after text redaction, before chunking.

**Result**: No human-readable client identifiers stored in Pinecone metadata when redaction is enabled.

---

## ‚úÖ Latest Updates (December 18, 2025) ‚Äî Mistral OCR Reliability Improvements

### Request-level timeouts (prevents ‚Äúhung worker‚Äù OCR calls)
- **Change**: `src/parsers/mistral_parser.py` now configures the Mistral SDK client with `timeout_ms`.
- **Why**: Unix `SIGALRM` timeouts do **not** reliably interrupt blocking HTTP calls inside the SDK; request-level timeouts ensure workers can‚Äôt wedge indefinitely.

### Large-PDF handling (include large PDFs safely)
- **Change**: PDFs **> 20MB** are automatically **split into page-range chunks** and OCR‚Äôd chunk-by-chunk, then stitched back together.
- **Defaults**:
  - `split_large_pdfs_over_mb = 20.0`
  - `split_pages_per_chunk = 10`
- **Why**: Large PDFs (e.g., `Re_ Tempur.pdf`) previously caused long stalls; chunking reduces worst-case latency and improves resumability.

### Failure behavior (safer than ‚Äúerror text‚Äù upserts)
- **Change**: Mistral OCR timeouts / API failures now **fail the document** instead of returning ‚Äúfallback error text‚Äù that could be chunked and upserted.

---

## ‚úÖ Code Quality Improvements (December 17, 2025)

### Pipeline Script Standardization
Comprehensive code quality improvements applied across key pipeline scripts:

**Files Updated:**
- `discover_documents.py` - Document discovery entrypoint
- `process_discovered_documents.py` - Document processing orchestrator
- `src/pipeline/parallel_processor.py` - Parallel processing engine
- `scripts/redaction_harness_local.py` - Redaction testing harness

**Improvements Implemented:**

1. **Security Fix (Critical)**
   - Removed API key from log output in `process_discovered_documents.py`
   - Command examples now use `os.getenv('OPENAI_API_KEY')` instead of hardcoded values

2. **Type Hints**
   - Added return type annotations to all public methods
   - Improves IDE support, type checking, and code documentation
   - Examples: `-> None`, `-> argparse.ArgumentParser`, `-> Dict[str, Any]`

3. **Google-Style Docstrings**
   - Standardized all docstrings with Args/Returns/Raises sections
   - Consistent documentation format across all modules
   - Better IDE tooltips and documentation generation

4. **Constants Extraction**
   - Extracted magic numbers to named constants:
     - `DEFAULT_BATCH_SIZE`, `DEFAULT_NAMESPACE`, `DEFAULT_REDACTION_MODEL`
     - `DEFAULT_MAX_CHUNK_SIZE`, `DEFAULT_CHUNK_OVERLAP`
     - `METADATA_TEXT_MAX_BYTES`, `WORKER_QUEUE_TIMEOUT_SECONDS`
   - Improves maintainability and reduces errors

5. **Refactoring**
   - Split `_convert_to_document_metadata()` (90+ lines) into focused helper methods:
     - `_extract_file_info()` - File metadata extraction
     - `_extract_business_metadata()` - Business/deal timing fields
     - `_extract_deal_metadata()` - Salesforce deal fields
     - `_extract_llm_classification()` - LLM classification data
   - Each helper is testable independently

6. **Module Exports**
   - Added explicit `__all__` declarations to control public API
   - Prevents accidental imports of private functions
   - Clearer module boundaries

7. **Test Coverage**
   - Created `tests/test_document_processor_helpers.py` with comprehensive tests
   - Tests cover all new helper methods with edge cases
   - Validates error handling and missing field scenarios

**Impact:**
- ‚úÖ Zero breaking changes - all improvements are backward compatible
- ‚úÖ Improved maintainability - easier to understand and modify code
- ‚úÖ Better IDE support - type hints enable better autocomplete and error detection
- ‚úÖ Enhanced documentation - standardized docstrings improve developer experience
- ‚úÖ All changes pass linting with no errors

**Next Steps:**
- Consider applying similar improvements to other modules (`src/connectors/`, `src/parsers/`, etc.)
- Add type hints to more internal methods as code evolves

### Parser reporting note (Word/Excel/etc.)
- The harness uses `DocumentConverter` to do the *real* extraction for non-PDFs (e.g. `.docx` via `python-docx`, `.doc` via `docx2txt`, spreadsheets via pandas/openpyxl).
- The harness now records this explicitly as **`content_parser`** in `redaction.json` and the review index so Word docs do **not** appear as ‚Äúdocling‚Äù.

### Latest redaction harness validation (5-PDF run)
- **Run folder**: `output/redaction_harness_20251216_223622/`
- **Inputs**: `tmp_redaction_discovery_sample_10_deals.json` with `--limit 5 --seed 1337`, Docling OCR on, strict-mode on, LLM person redaction enabled
- **Result**: `attempted=5, succeeded=5, validation_failed=0`
- **Spot-check**: verified ‚Äúclient placeholder tail collapse‚Äù on the Denver Health doc (no residual `"... and Hospitals Authority Inc"` tail in `redacted.md`)


---

## ‚úÖ Latest Updates (December 13, 2025) - Session 5

### Production Processing Run (86,503 Documents)
- **Discovery completed**: 86,503 documents from organized Salesforce export (August 2025)
- **Processing started**: Background run with 6 workers, Mistral parser
- **Index**: `npi-deal-data`, **Namespace**: `sf-export-aug15-2025`
- **Metadata update (Dec 15, 2025)**: Added numeric Unix timestamp fields for reliable Pinecone date range filters:
  - `deal_creation_date_ts`, `contract_start_ts`, `contract_end_ts` (keep string fields too)

### Critical Learnings: Discovery JSON `source_path` Issue

**‚ö†Ô∏è PROBLEM**: When using `--source salesforce`, the discovery script may not set `source_path` in the metadata, causing processing to fail with:
```
ValueError: Discovery file missing source_path in metadata and could not be inferred.
```

**‚úÖ FIX**: Manually update the discovery JSON after discovery completes:
```python
import json
with open('production_august_discovery_12_13_2025.json', 'r') as f:
    d = json.load(f)
d['discovery_metadata']['source_path'] = '/Volumes/Jeff_2TB/organized_salesforce_v2'
d['discovery_metadata']['salesforce_files_dir'] = '/Volumes/Jeff_2TB/organized_salesforce_v2'
with open('production_august_discovery_12_13_2025.json', 'w') as f:
    json.dump(d, f, indent=2, default=str)
```

### Correct Discovery Command (August 2025 Export)
```bash
AUGUST_CSV="/Volumes/Jeff_2TB/WE_00D80000000aWoiEAE_1"

python discover_documents.py --source salesforce \
  --salesforce-files-dir /Volumes/Jeff_2TB/organized_salesforce_v2 \
  --file-mapping-csv organized_files_to_deal_mapping_enhanced.csv \
  --deal-metadata-csv $AUGUST_CSV/Deal__c.csv \
  --client-mapping-csv $AUGUST_CSV/SF-Cust-Mapping.csv \
  --vendor-mapping-csv $AUGUST_CSV/SF-Vendor_mapping.csv \
  --require-deal-association \
  --output production_august_discovery.json
```

**Key Notes:**
- `--file-mapping-csv` is in the **workspace directory** (not on external drive)
- CSV metadata files are in `$AUGUST_CSV` directory on external drive
- `--require-deal-association` excludes unmapped files

### Correct Processing Command
```bash
python process_discovered_documents.py \
  --input production_august_discovery_12_13_2025.json \
  --namespace sf-export-aug15-2025 \
  --parser-backend mistral \
  --workers 6
```

### Discovery File Order (NOT Alphabetical!)
**Discovery uses `os.walk()` which returns directories in filesystem order, NOT alphabetical.**

The actual processing order for the August 2025 export:
| Order | Folder | First Doc # | Count |
|-------|--------|-------------|-------|
| 1 | `images/` | #1 | 506 |
| 2 | `emails/` | #507 | 16,469 |
| 3 | `spreadsheets/` | #16,976 | 18,854 |
| 4 | `other/` | #35,830 | 6 |
| 5 | `presentations/` | #35,836 | 902 |
| 6 | `documents/` | #36,738 | 49,766 |

**Key insight:** PDFs/Word docs (in `documents/`) don't start processing until doc #36,738 (~43% through).

The order reflects when folders were created on disk, not alphabetical. This is in `src/connectors/salesforce_file_source.py` line 494:
```python
for root, _, files in os.walk(search_path):  # No sorting!
```

### JSON Corruption Fix (Atomic Writes)
- **Problem**: Parallel workers corrupted the discovery JSON when interrupted (Ctrl+C)
- **Solution**: Implemented atomic writes in `src/utils/discovery_persistence.py`:
  - Write to temp file first
  - Use `fsync()` to ensure data hits disk
  - Atomic rename (`shutil.move`) to final path
  - Batched updates (save every 50 documents, not every document)
  - `flush_updates()` called during graceful shutdown

---

## üìã TODO: Known Edge Cases to Fix

### Special Character Encoding in Filenames (Low Priority)
- **Issue**: Files with special characters (e.g., `√∫`, `√©`, `√±`) in filenames cause Pinecone upsert failures
- **Example**: `emails/RE FMV feedback- Internet Services Pe√∫  Ecuador.msg` ‚Üí `Pinecone upsert failed`
- **Impact**: ~0.02% of documents (1 out of 4,000+ in current run)
- **Fix needed**: Add UTF-8 encoding sanitization in `src/connectors/pinecone_client.py` before upsert
- **Workaround**: Files are skipped and logged; can be reprocessed later

---

## ‚úÖ Latest Updates (December 13, 2025) - Session 4

### Pinecone Index/Namespace Document Comparator (NEW)
- **Added `scripts/compare_pinecone_targets/compare_pinecone_targets.py`** - Tool to compare documents between two Pinecone indexes/namespaces
- **Features**: Document resolution (deal_id/file_name), chunk reconstruction, text diagnostics, embedding comparison
- **Outputs**: JSON + Markdown reports with side-by-side diagnostics and reconstructed text files
- **Documentation**: Complete README in `scripts/compare_pinecone_targets/README.md`
- **Patterns**: Uses safe pagination (`list()` + `fetch()`), `_sanitize_str()` for null safety, schema-adaptive text extraction

### Pricing/Trend Evaluation Harness (NEW)
- **Extended `scripts/compare_pinecone_targets/sample_25_deals_harness.py`** with:
  - `--pricing-eval`: deterministic pricing signals (currency/percent/year density, numeric token density, pricing keywords)
  - `--pricing-judge`: GPT‚Äë5.2 pricing-focused judge rubric (prices, trends, comparisons)
- Output includes per-PDF `pricing_signals` + `pricing_judgment` in `manifest.json`, plus aggregate deltas + winner counts in `summary.md`

**Usage:**
```bash
python scripts/compare_pinecone_targets/compare_pinecone_targets.py \
  --left-index npi-deal-data --left-namespace test-namespace \
  --right-index npi-deal-data --right-namespace production-namespace \
  --deal-id "58773" --file-name "contract.pdf"
```

**Key Capabilities:**
- Resolves documents by deal_id/file_name with exact filter + scan fallback
- Reconstructs full text from chunks with section markers
- Computes LLM-usefulness diagnostics (OCR artifacts, table preservation, etc.)
- Compares embeddings (centroid similarity when dimensions match)
- Generates machine-readable JSON + human-readable Markdown reports

---

## ‚úÖ Latest Updates (December 13, 2025) - Session 3

### Deal Creation Date Filtering (NEW)
- **Added `--deal-created-after` and `--deal-created-before`** CLI options for reliable date filtering
- Uses **authoritative `deal_creation_date`** from Salesforce `Deal__c.csv` (not unreliable disk file dates)
- Date format support: `M/D/YY HH:MM` (Salesforce), `YYYY-MM-DD` (CLI input)
- **Files modified**: `src/utils/discovery_persistence.py`, `process_discovered_documents.py`, `src/pipeline/parallel_processor.py`

**Usage:**
```bash
# Process only documents from deals created in 2000 or later
python process_discovered_documents.py \
  --input discovery.json \
  --deal-created-after 2000-01-01 \
  --parser-backend mistral \
  --parallel --workers 8
```

### Enhanced Discovery Summary
- **Added `--show-summary`** option to view detailed statistics of existing discovery files
- Shows: year distribution, file type breakdown, date range analysis, size statistics
- **Files modified**: `discover_documents.py`, `src/utils/discovery_persistence.py`

### Enhanced Processing Logging
- **File type breakdown**: Count, success/failure, avg time, chunks per file type
- **Error categorization**: timeout, download_failed, no_text_extracted, unsupported_type, pinecone_error
- **Timing percentiles**: P50, P90, P99, fastest, slowest
- **Files modified**: `src/pipeline/parallel_processor.py`

### Removed `description` Field from Pinecone Metadata
- **Removed** long-text `description` field (not suitable for filtering)
- Now **29 metadata fields** (was 30)
- **Files modified**: `src/connectors/pinecone_client.py`, `src/connectors/raw_salesforce_export_connector.py`, `src/models/document_models.py`, `src/pipeline/parallel_processor.py`, `process_discovered_documents.py`

### Resume Capability Tested ‚úÖ
- Successfully tested 3-run resume scenario (5 ‚Üí 10 ‚Üí 15 documents)
- Correctly excludes already-processed documents (`excluded_processed=N`)
- Progress persisted to discovery JSON after each document

---

## ‚úÖ Latest Updates (December 13, 2025) - Session 2

### Metadata Sanitization Fix
- **Fixed `nan`/`None` string pollution** in Pinecone metadata - added `_sanitize_str()` helper in `src/connectors/pinecone_client.py`
- Converts Python `None`, float `NaN`, and strings `"None"/"nan"/"NaN"/"null"` to empty strings
- **Result**: Clean metadata in Pinecone (no more `contract_term: "nan"` or `client_name: "None"`)

### Enhanced Parser Tracking
- **Added `parser_backend` field** to `processing_status` - tracks PDF parser selection (mistral, docling, pdfplumber)
- **Added `content_parser` field** to `processing_status` - tracks **actual parser** used per file type:
  - `.pdf` ‚Üí `mistral`, `docling`, or `pdfplumber`
  - `.xlsx`, `.xls`, `.csv` ‚Üí `pandas_openpyxl`
  - `.docx` ‚Üí `python_docx`
  - `.doc` ‚Üí `docx2txt`
  - `.msg` ‚Üí `extract_msg`
  - `.pptx` ‚Üí `python_pptx`
  - `.txt` ‚Üí `direct_text`
- **Files modified**: `process_discovered_documents.py`, `src/pipeline/parallel_processor.py`, `src/utils/discovery_persistence.py`

### Full Pipeline Test Success
- **20-document end-to-end test** completed successfully (discovery ‚Üí processing ‚Üí Pinecone)
- **Index**: `npi-deal-data`, **Namespace**: `test-full-pipeline-20docs-20251213`
- **Results**: 20/20 documents, 380 chunks, clean metadata, all parsers tracked

### New Processing Status Schema
```json
{
  "processed": true,
  "processing_date": "2025-12-13T...",
  "processor_version": "2.0",
  "parser_backend": "mistral",      // PDF parser selection
  "content_parser": "extract_msg",  // Actual parser for file type
  "chunks_created": 4,
  "vectors_created": 4,
  "pinecone_namespace": "...",
  "processing_errors": [],
  "processing_time_seconds": 0.68
}
```

## ‚úÖ Latest Updates (December 17, 2025) - Parser Default Change
- **Changed default PDF parser to Mistral OCR** (`--parser-backend mistral` is now the default)
- Previous default was `pdfplumber`; Mistral provides better production quality for scanned/image PDFs
- Other options: `--parser-backend docling` (best tables) or `--parser-backend pdfplumber` (fast, local)

## ‚úÖ Latest Updates (December 13, 2025) - Session 1
- Enabled `--parser-backend mistral` in `process_discovered_documents.py` (works in both serial and `--workers N` parallel runs).
- Added Mistral support to `src/pipeline/parallel_processor.py` worker initialization (each worker creates its own `MistralParser` client).
- Fixed argparse help formatting crash by escaping `50%%` in `--use-batch` help text.

## ‚úÖ Latest Updates (December 12, 2025)
- Fixed Phase 2 selection logic so `process_discovered_documents.py` filters actually work (file types, modified date range, min/max size) and are applied consistently in both serial runs and `--workers N` parallel runs.
- Default exclusion now supports skipping `.png` at selection time (avoids wasting processing cycles on images).
- Discovery for `--source salesforce_raw` now persists `export_root_dir` into discovery metadata so parallel processing can reliably locate files on EC2 (no hardcoded `/Volumes/...` fallback).
- Removed remaining "production footguns" in `process_discovered_documents.py`: deleted hardcoded Salesforce `/Volumes/...` paths and replaced with CLI/env configuration + clear errors when missing.

## ‚úÖ Latest Updates (December 10, 2025)
- Added Mistral OCR parser (optional) with CLI selection; dependency `mistralai==1.7.0` and `MISTRAL_API_KEY` configured.
- Benchmarked parsers on local 10-file set: pdfplumber ~1.37s avg, Mistral ~12.56s avg, Docling ~50.77s avg. Docling remains canonical for structured/BOM/pricing data; Mistral better for cosmetic context but noisier‚Äîdo not overwrite Docling numeric fields with Mistral.
- Test harness updated (`test_local_dec10/run_local_test.py --parser all`) writes per-parser outputs and summary; venv at `venv` (activate before running).

## ‚úÖ Latest Updates (December 5, 2025)

### Client/Vendor Mapping Files
- ‚ö†Ô∏è **NOT auto-generated** with Salesforce exports
- **Copied from older export**: `SF-Cust-Mapping.csv` (955 clients), `SF-Vendor_mapping.csv` (6,259 vendors)
- **New unified location**: `/Volumes/Jeff_2TB/day_20251202_053804-04f8f8_29771/`
- **Documented in**: `memory-bank/RAW_SALESFORCE_EXPORT_FORMAT.md`
- **TODO**: Consider creating Salesforce API script to auto-generate these

### Simplified Metadata Schema
- Reduced from 53 fields to **22 fields** in Pinecone
- Removed: `client_id`, `vendor_id`, `parser_backend`, `processing_method`
- **Text storage**: Chunk text **is stored in Pinecone metadata** as `text`, truncated to ~37KB to stay under Pinecone‚Äôs per-record metadata limit.
- Client/vendor names preferred over IDs

### Production Processing Setup
- **Index**: `npi-deal-data` (created with `dotproduct` metric for hybrid search)
- **Namespace**: `salesforce-extract-2025-12-05`
- **Parser**: Docling (GPU-accelerated on Apple Silicon via MPS)
- **Parallel processing**: Implemented with `--workers N` flag

### Discovery Commands (Use This!)
```bash
EXPORT_DIR="/Volumes/Jeff_2TB/day_20251202_053804-04f8f8_29771"
python discover_documents.py --source salesforce_raw \
  --export-root-dir $EXPORT_DIR \
  --content-versions-csv $EXPORT_DIR/content_versions.csv \
  --content-documents-csv $EXPORT_DIR/content_documents.csv \
  --content-document-links-csv $EXPORT_DIR/content_document_links.csv \
  --deal-metadata-csv $EXPORT_DIR/deal__cs_fully_merged_financials.csv \
  --client-mapping-csv $EXPORT_DIR/SF-Cust-Mapping.csv \
  --vendor-mapping-csv $EXPORT_DIR/SF-Vendor_mapping.csv \
  --require-deal-association \
  --output production_discovery.json

# View summary of existing discovery file
python discover_documents.py --source local --path . \
  --output production_discovery.json --show-summary
```

### Processing Commands with Date Filtering
```bash
# Process all documents from deals created 2000 or later (recommended)
python process_discovered_documents.py \
  --input production_discovery.json \
  --deal-created-after 2000-01-01 \
  --namespace production \
  --parser-backend mistral \
  --parallel --workers 8 \
  --resume

# Selection summary will show:
# excluded_deal_date_missing=X  (docs without deal_creation_date)
# excluded_deal_after=Y         (docs with deals before 2000)
```

---

## Previous Updates

### ‚úÖ Marketing namespace uploader (November 5, 2025)
- Added `upload_marketing_resources.py` to support simple local-folder ingestion to Pinecone with minimal metadata.
- Reuses existing `DocumentConverter`, `PDFPlumberParser`, and `SemanticChunker` for consistent processing, with hybrid embeddings generated via `PineconeDocumentClient`.
- Progress tracking and summaries powered by `ProcessingProgressLogger`; designed for new `marketing-resources` namespace with filename/modified-date metadata only.

### ‚úÖ Latest Update: Metadata pruning + client cleanup (August 23, 2025)
- Removed low-value metadata from Pinecone writes: `content_summary`, `key_topics`, `vendor_products_mentioned`, `pricing_indicators` (kept full text for semantic search; reduced metadata size/risk)
- Updated all code paths: `PineconeDocumentClient`, `DocumentProcessor`, models, classifiers, scripts, and docs
- Deleted legacy `src/connectors/optimized_pinecone_client.py` and updated docs to standardize on `PineconeDocumentClient`
- Verified lints green after changes

### ‚ñ∂Ô∏è Next Steps: LLM enrichment plan (pilot then scale)
1) Pilot scope: 5,000 documents in `business-documents` / `SF-Files-2020-8-15-25`
2) Group by `document_path`; sample first chunk text (‚â§1500 chars)
3) Call `LLMDocumentClassifier.classify_document_enhanced()` (pruned schema); write back via `index.update(set_metadata=...)`
4) Only update docs where `classification_method` is rule-based/missing; keep `document_type` normalized (5 classes)
5) Safety: truncate strings, arrays; respect 40KB metadata limit (now lower risk after pruning)
6) Validate via Streamlit search (filters: document_type, pricing/terms depths, term dates)
7) If pilot is good, scale to full namespace with batch collection option

### üß™ Testing note
### ‚úÖ Latest Update: PowerPoint VLM Integration (August 27, 2025)
- Code-level enablement complete: `DocumentProcessor(..., openai_client, enable_vision_analysis=True, vision_model="gpt-4o")`.
- Enhanced PPTX parser now extracts images and calls OpenAI Vision; recommended model: **gpt-4o** (or **gpt-4o-mini** for cost).
- Limitation: charts/diagrams aren‚Äôt rasterized by python-pptx; analysis returns informative note. Slide image rendering is a possible enhancement.
- Next: Decide whether to wire VLM on by default in `process_enhanced_salesforce_direct.py` or keep it opt-in.
- After changes, start a new chat/session and verify search UI still loads results, and filters work with pruned fields

### ‚úÖ Latest Update: LLM Metadata Enrichment via OpenAI Batch (August 25, 2025)
- Namespace: `SF-Files-2020-8-15-25`
- Batch collection and application:
  - enrich_1: 15,000 docs ‚Äì completed and upserted
  - enrich_2: 15,000 docs ‚Äì completed and upserted
  - enrich_3: 15,000 docs ‚Äì completed; upsert in progress/running
  - enrich_4: 15,000 docs ‚Äì completed; upsert in progress/running
  - enrich_5: 3,041 docs ‚Äì submitted (final slice from manifest)
- Coverage sample (n=5,000 chunks):
  - document_type, document_type_confidence, classification_method: 100%
  - product_pricing_depth & commercial_terms_depth: ~73.1%
  - proposed_term_start: ~19.1%; proposed_term_end: ~13.7%

### üîé Legacy Document Type Cleanup (August 25, 2025)
- Detected legacy/un-normalized types present alongside normalized ones: `document`, `email`, `proposal`, `fmv_report`
- Built targeted manifest of legacy documents for re-enrichment:
  - `legacy_manifest.txt` ‚Üí 28,909 `document_path` entries
  - Progress logging + checkpointing enabled during manifest build
- Re-enrichment plan: batch windowing over `legacy_manifest.txt` using `enrich_pinecone_metadata.py`, apply results with `batch_processing_updater.py`.

### üß∞ New/Updated Operational Scripts
- `scripts/enrich_pinecone_metadata.py`
  - Purpose: Post-ingest LLM enrichment of existing Pinecone documents (no re-parsing/re-chunking)
  - Modes: Immediate (direct LLM) or `--use-batch` (OpenAI Batch, 50% cost)
  - Inputs: existing chunks (builds preview from multiple chunks)
  - Features: `--manifest`, `--start-index`, `--limit` for deterministic batch windows; preview strategies (`head_mid_tail` default, or `first_n_chunks`); `--preview-budget` (default 4000)
- `batch_processing_updater.py`
  - Purpose: Monitor batch jobs, retrieve results, and upsert enhanced metadata via `index.update(set_metadata=...)` across all chunks by `document_path`
  - Artifacts: `batch_job_index.json` (job ‚Üí mapping file), `batch_mapping_*.json` (custom_id ‚Üî document_path)
- `scripts/build_legacy_types_manifest.py`
  - Purpose: Iterate vectors, collect `document_path` for legacy `document_type` values (contract, document, email, proposal, fmv_report)
  - Features: Progress logs (processed/found/rate), periodic checkpoint `legacy_manifest.partial.txt`, final `legacy_manifest.txt`
- `streamlit-ui/app_standalone.py`
  - Update: Configurable model via `STREAMLIT_LLM_MODEL` (default `gpt-5-mini-2025-08-07`), GPT‚Äë5 Responses API support (`max_output_tokens`), automatic fallback to `gpt-4.1-mini` if empty content
  - Sidebar shows active model; removed deprecated params for GPT‚Äë5 (uses `max_completion_tokens`)
- Rolling Re-embedding (existing): `scripts/reembed_namespace.py`
  - Purpose: In-place re-embedding of existing vectors with `input_type="passage"` using stored `metadata.text` (no LLM calls), preserving ids/metadata

### üìà Current Batch Windowing State
- Primary manifest (`enrich_manifest.txt`): 63,041 docs ‚Üí processed in 15K windows (enrich_1..4) + final 3,041 (enrich_5)
- Legacy manifest (`legacy_manifest.txt`): 28,909 docs ‚Üí scheduled for two windows (15K + 13,909)

## üéâ **LATEST BREAKTHROUGH: VENDOR METADATA POPULATION COMPLETED - September 5, 2025**
**Status**: ‚úÖ **CRITICAL BUG FIXED & PRODUCTION RUNNING** - Vendor metadata population successfully executing after string detection fix

### **üöÄ VENDOR METADATA POPULATION - CRITICAL STRING DETECTION BUG FIXED**

#### **üîß CRITICAL BUG DISCOVERY & RESOLUTION - September 5, 2025**
**CRITICAL BUG IDENTIFIED**: `populate_vendor_ids_fixed.py` had string detection logic error treating `vendor_id: "None"` (string) as populated data.

**Root Cause Analysis**:
- Documents contained `vendor_id: "None"` as STRING values (not null/empty)
- Script logic: `if vendor_id and str(vendor_id).strip():` treated `"None"` as truthy
- Result: 99.6% of documents incorrectly identified as "already populated"
- SQLite analysis showed 1.85% coverage, but script showed 100% - major discrepancy

**‚úÖ COMPLETE STRING DETECTION FIX IMPLEMENTED**:
```python
# BEFORE (broken):
if vendor_id and str(vendor_id).strip():
    batch_stats['already_populated'] += 1

# AFTER (fixed):
vendor_id_clean = str(vendor_id).strip() if vendor_id else ''
if vendor_id_clean and vendor_id_clean.lower() not in ['none', 'nan', '']:
    batch_stats['already_populated'] += 1
```

#### **üìä VALIDATION RESULTS - September 5, 2025**
**Analysis Results BEFORE Fix**:
- `missing_vendor_id: 0/1000 (0.0%)` ‚ùå
- `already_has_vendor_id: 1000/1000 (100.0%)` ‚ùå
- **Script would process 0 documents**

**Analysis Results AFTER Fix**:
- `missing_vendor_id: 996/1000 (99.6%)` ‚úÖ
- `mappable_via_deal: 996/1000 (99.6%)` ‚úÖ  
- `already_has_vendor_id: 4/1000 (0.4%)` ‚úÖ
- **Perfect alignment with SQLite analysis (1.85% coverage)**

#### **üìà PRODUCTION EXECUTION STATUS**
**Current Production Run**: ‚úÖ **ACTIVE** - Vendor metadata population in progress
- **Namespace**: `SF-Files-2020-8-15-25`
- **Total Documents**: ~1.26M (from SQLite analysis)
- **Expected Updates**: ~1.24M documents (99.6% missing vendor data)
- **Mapping Sources**: 36,808 deal mappings + 6,259 vendor name mappings loaded
- **Script**: `populate_vendor_ids_fixed.py` with string detection fix applied

#### **üéØ EXPECTED FINAL RESULTS**
- **Coverage Improvement**: 1.85% ‚Üí ~98%+ vendor_id/vendor_name population
- **Documents Enhanced**: ~1.24M documents receiving proper vendor metadata
- **Streamlit UI**: Complete vendor/client filtering functionality restored
- **SQLite Validation**: Post-run analysis will confirm coverage improvement

### **üîß PREVIOUS: CRITICAL SOURCE CONFIGURATION DISCOVERY**
**ROOT CAUSE IDENTIFIED**: The SF-Files-2020-8-15-25 namespace was processed using `--source local` instead of `--source salesforce`, resulting in missing vendor_id fields and 30+ metadata fields. This configuration error has been:
- ‚úÖ **Documented** in README.md with clear source selection guide
- ‚úÖ **Fixed Retroactively** with vendor enhancement script (26,000+ vendor IDs populated)
- ‚úÖ **Prevented Future Issues** with configuration warnings and validation
- ‚úÖ **Script Fixed** - Infinite loop resolved with proper Pinecone pagination

### **üìÇ SOURCE CONFIGURATION GUIDE**

| Source Type | Use Case | Metadata Fields | vendor_id | Command |
|-------------|----------|-----------------|-----------|---------|
| **`--source salesforce`** | ‚úÖ **Salesforce files** | **37+ rich fields** | ‚úÖ YES | Full Deal metadata + financial + narrative |
| **`--source dropbox`** | Dropbox folders | 6 basic fields | ‚ùå NO | File info + basic path extraction |
| **`--source local`** | Local directories | 6 basic fields | ‚ùå NO | File info + basic path extraction |

### **‚ö†Ô∏è CRITICAL: Always Use Correct Source**
```bash
# ‚úÖ CORRECT for Salesforce files
python discover_documents.py --source salesforce \
  --salesforce-files-dir "/path/to/organized_salesforce_v2" \
  --file-mapping-csv "organized_files_to_deal_mapping_enhanced.csv" \
  --deal-metadata-csv "/path/to/Deal__c.csv"

# ‚ùå WRONG for Salesforce files (misses 30+ fields!)
python discover_documents.py --source local --path "/organized_salesforce_v2"
```

## üéâ **PREVIOUS: ENHANCED SALESFORCE DIRECT PROCESSING SYSTEM - August 21, 2025**
**Status**: ‚úÖ **PRODUCTION READY** - Direct processing from enhanced JSON with 90%+ pipeline leverage

### **üöÄ Direct Processing Implementation Achievements**
- **‚úÖ Enhanced JSON Analysis**: 85,404 documents with 57+ metadata fields in nested structure
- **‚úÖ Field Optimization**: Reduced from 44 to 27 fields (38.6% reduction) by eliminating duplicates
- **‚úÖ EnhancedSalesforceFileSource**: New file source with optimized metadata extraction
- **‚úÖ Direct Processing Script**: `process_enhanced_salesforce_direct.py` with full pipeline integration
- **‚úÖ Production Logging**: ProcessingProgressLogger with real-time ETA and structured monitoring
- **‚úÖ Resume Capability**: Automatic state saving every 100 documents with perfect resume functionality

### **üéØ Performance Validation Results**
- **üíØ 100% Success Rate**: All test documents processed successfully
- **‚ö° 104 docs/min**: Stable processing rate with rich content extraction
- **üß© 6.5 chunks/doc**: Average chunk creation from business emails and documents
- **üìä Rich Metadata**: 27 optimized fields including deal IDs, financial data, content summaries
- **üîÑ Resume Tested**: Successfully resumed from interruption points without data loss

### **üìà Production Processing Strategy**
**Phase 1: High-Value Documents (64,123 docs - 10.3 hours)**
```bash
python process_enhanced_salesforce_direct.py \
    --file-types .pdf .docx .msg \
    --namespace salesforce-enhanced-2025
```
- **Core Business Documents**: Contracts, emails, reports (75% of total dataset)
- **Rich Content**: PDF reports, Word contracts, email communications
- **Business Intelligence**: Deal metadata, financial data, content summaries

**Phase 2: Financial Data (18,127 docs - 3 hours)**
```bash
python process_enhanced_salesforce_direct.py \
    --file-types .xlsx .xls .csv \
    --namespace salesforce-enhanced-2025 \
    --resume
```
- **Financial Spreadsheets**: Pricing data, financial analysis, deal metrics
- **Structured Data**: Excel workbooks with financial intelligence

**Phase 3: Presentations & Others (3,154 docs - 30 minutes)**
```bash
python process_enhanced_salesforce_direct.py \
    --file-types .pptx .eml \
    --namespace salesforce-enhanced-2025 \
    --resume
```

### **üèóÔ∏è Architecture Excellence**
- **90%+ Pipeline Leverage**: Reuses existing DocumentProcessor, PineconeClient, SemanticChunker
- **Zero Discovery Files**: Direct processing from enhanced JSON eliminates intermediate files
- **Minimal Code Duplication**: Only JSON mapping and resume logic added (~10% custom code)
- **Production Monitoring**: Full ProcessingProgressLogger integration with structured logging

### **üéØ Ready for Production Execution**
- **85,404 total documents** with pre-enriched metadata ready for processing
- **13.7 hours estimated** for complete dataset (can be split across multiple sessions)
- **Perfect resume capability** allows interruption and continuation at any point
- **Rich business intelligence** preserved throughout processing pipeline

## üéØ **PREVIOUS ACHIEVEMENT: VLM POWERPOINT INTEGRATION + ENHANCED DISCOVERY SYSTEM**

### **üéâ VLM PowerPoint Integration Completed - August 18, 2025**
**Status**: ‚úÖ **FULLY COMPLIANT** - OpenAI Vision API integration ready for PowerPoint processing

#### **üöÄ VLM Implementation Achievements**
- **‚úÖ OpenAI Vision API Compliance**: Updated from deprecated "gpt-4-vision-preview" to production "gpt-4o"
- **‚úÖ High Detail Analysis**: Added "detail": "high" parameter for business documents and charts
- **‚úÖ Configurable Models**: Support for gpt-4o, gpt-4o-mini, gpt-4-turbo selection
- **‚úÖ Enhanced Error Handling**: Comprehensive logging with image size debugging
- **‚úÖ DocumentConverter Integration**: VLM capabilities integrated into processing pipeline
- **‚úÖ Framework Complete**: Ready for 1,116 PowerPoint files in organized Salesforce dataset

### **üéâ Enhanced Discovery System Completed - August 18, 2025**
**Status**: ‚úÖ **COMPLETE** - 112,965 documents with enhanced client/vendor name mapping

#### **üéØ Enhanced Discovery Achievements**
- **‚úÖ Complete File Discovery**: 112,965 files from organized Salesforce dataset
- **‚úÖ Enhanced Mapping Integration**: Client/vendor names (99.4% and 99.9% coverage)
- **‚úÖ Business Intelligence Ready**: Deal metadata with actual company names
- **‚úÖ File Type Distribution**: 51,997 PDFs, 21,323 Excel, 19,365 MSG, 16,284 DOCX, 1,120 PPTX
- **‚úÖ Salesforce Integration**: Fixed client/vendor name resolution in SalesforceFileSource
- **‚úÖ Discovery Verification**: Enhanced mapping working with 100% name coverage tested

### **üéâ Enhanced LLM Processing Success - August 18, 2025**
**Status**: ‚úÖ **COMPLETE** - Enhanced metadata now populating with AI-generated content summaries

#### **üéØ Enhanced Processing Achievements**
- **‚úÖ Vendor/Client Names Fixed**: Real names now mapped (Salesforce, T-Mobile, Intel, etc.)
- **‚úÖ Enhanced LLM Classification**: AI-generated content summaries, document types, pricing depth
- **‚úÖ 60 Metadata Fields**: Complete business intelligence with enhanced AI analysis
- **‚úÖ Production Validation**: 3 documents successfully processed with 85-95% confidence
- **‚úÖ Pinecone Updates**: Enhanced metadata successfully stored in salesforce-deals-2025 namespace

#### **üîß Namespace Discovery**
- **salesforce-deals-2025**: 826 enhanced deal documents (emails, PDFs, DOCX)
- **documents**: 103K+ documents including 21 Salesforce Excel files
- **Search Gap Identified**: Streamlit searches single namespace, Excel files in different namespace

### **üßπ Environment Cleanup - August 18, 2025**
**Status**: ‚úÖ **COMPLETE** - Clean production environment established

#### **‚úÖ Cleanup Achievements**
- **Temporary Files Removed**: 8 testing scripts deleted
- **Analysis Files Cleaned**: Redundant JSON/MD files removed  
- **Production Structure**: Clean file organization established
- **Utility Scripts Organized**: vendor/client updater moved to scripts/utilities/
- **Documentation Consolidated**: Production file structure documented

## üéØ **PREVIOUS ACHIEVEMENT: HYBRID SEARCH VALIDATION SUCCESS**

### **üéØ Salesforce Dataset Hybrid Search Validation - August 17, 2025**
**Status**: ‚úÖ **MAJOR SUCCESS** - Hybrid search functioning perfectly with critical technical discoveries

#### **üéâ Validation Achievements**
- **‚úÖ 100% Query Success Rate**: All 8 natural language queries returned results  
- **‚úÖ Hybrid Embeddings Working**: Dense (multilingual-e5-large) + Sparse (pinecone-sparse-english-v0) confirmed
- **‚úÖ Semantic Chunking Operational**: LangChain semantic chunking processing successfully
- **‚úÖ Curated Dataset Quality**: 93 high-quality Salesforce files with 236 chunks generated
- **‚úÖ Rate Limiting Handled**: Pinecone API limits gracefully managed with retry logic
- **‚úÖ Index Metric Discovery**: Critical finding - hybrid search requires dotproduct metric (not cosine)

#### **üîß Critical Technical Discoveries**
- **Dotproduct Metric Requirement**: Hybrid search (sparse + dense) requires dotproduct similarity metric
- **Index Configuration**: Successfully created `salesforce-curated-dotproduct-2025-01-31` index 
- **Processing Pipeline**: Semantic chunking + hybrid embeddings working seamlessly
- **Rate Limiting**: Expected 429 errors during bulk processing, handled by retry logic
- **Content Filtering**: Short content automatically filtered out by semantic chunker

#### **üéâ CRITICAL ISSUE RESOLVED: Deal Metadata Fully Operational**
**Resolution Date**: August 17, 2025
**Status**: ‚úÖ **COMPLETE SUCCESS** - Deal-aware natural language search fully operational

**Root Causes Fixed**:
1. **Duplicate DocumentMetadata Classes**: Removed old version in `dropbox_client.py`, unified to `models.document_models.DocumentMetadata`
2. **Missing Upload Fields**: Added all 20+ Salesforce deal metadata fields to `upsert_chunks()` method  
3. **Missing Search Fields**: Added deal metadata fields to `DocumentSearchResult` class

**Production Validation**:
- ‚úÖ **"Show me deals over 30 million dollars"** ‚Üí Found $45.9M and $58.9M deals
- ‚úÖ **"What are the highest value Salesforce renewals?"** ‚Üí Found $43.2M and $58.9M renewals  
- ‚úÖ **"Show me Molina Health deal details"** ‚Üí Found multiple deals with correct amounts
- ‚úÖ **Full Dataset Processed**: 93 curated Salesforce files in `salesforce-deals-2025` namespace

**Business Intelligence Capabilities**:
Users can now perform sophisticated financial queries like:
- "What's our largest Salesforce renewal?" 
- "Find deals over $10 million"
- "Show me expired contracts that need renewal"
- "What deals did [client] negotiate?"

## üéâ **LATEST ACHIEVEMENT: LLM ENHANCEMENT SYSTEM READY FOR EXECUTION - January 31, 2025**
**Status**: ‚úÖ **OPTIMIZED & READY** - Complete parallel filtered enhancement system ready for unmapped documents

### **Enhanced Discovery System Status**:
- ‚úÖ **126,000+ documents discovered** from organized Salesforce dataset  
- ‚úÖ **77.4% mapping success** (81,287 deals mapped with full metadata)
- ‚úÖ **Client/vendor names integrated** via enhanced CSV mapping (99.4% and 99.9% coverage)
- ‚úÖ **25,950 unmapped documents identified** ready for LLM enhancement
- ‚úÖ **Discovery output**: `/Volumes/Jeff_2TB/organized_salesforce_v2_enhanced_discovery.json`

### **LLM Enhancement System Ready**:
- ‚úÖ **Parallel Processing**: `integrate_enhanced_discovery_parallel_filtered.py` - 8 concurrent OpenAI workers
- ‚úÖ **Smart Image Filtering**: Excludes 1,619 low-value files (6.2%) - logos, small images, generic patterns
- ‚úÖ **Optimized Processing**: 24,331 meaningful documents (93.8% of unmapped) will be enhanced
- ‚úÖ **Cost Optimization**: ~$7.30 total cost, saves $0.49 from intelligent filtering
- ‚úÖ **Business Intelligence Extraction**: LLM-powered client/vendor/deal inference from content

### **Enhancement Strategy Architecture**:
- ‚úÖ **BusinessIntelligenceExtractor**: GPT-4o-mini powered analysis engine
- ‚úÖ **Confidence Thresholds**: 70% client/vendor, 60% deal confidence for metadata updates
- ‚úÖ **Thread-Safe Processing**: Parallel workers with progress tracking every 50 documents
- ‚úÖ **Intermediate Saves**: Progress saved every 500 documents for resilience
- ‚úÖ **Expected Results**: 15-25% enhancement rate, boosting overall coverage to 80-82%

### **Ready for Execution**:
- üéØ **Command**: `python integrate_enhanced_discovery_parallel_filtered.py`
- üéØ **Expected Time**: ~51 minutes with 8 parallel workers
- üéØ **Output**: Enhanced discovery JSON with extracted business intelligence
- üéØ **Next Phase**: Phase 2 processing with VLM PowerPoint analysis and full pipeline

**üìä OPTIMIZATION COMPLETE**: Filtered approach processes 93.8% of meaningful content while saving 6.2% in costs and time

---

## üéâ **BREAKTHROUGH SUCCESS: 2022 PRODUCTION RUN COMPLETED**

### **üèÜ PHENOMENAL RESULTS - 4,385 Document Processing Campaign**
**Date**: July 26, 2025 (17:37 - 19:45, ~8 hours)  
**Status**: ‚úÖ **MASSIVE SUCCESS - 98.5% Success Rate**

#### **üìä Final Production Results:**
- **‚úÖ Documents Processed**: **4,319 out of 4,385** (98.5% success rate)
- **üß© Chunks Created**: **31,487 searchable chunks** uploaded to Pinecone
- **üéØ Vectors Generated**: **31,487 vectors** in `documents` namespace
- **‚ùå Failures**: Only **66 documents** with errors (1.5% failure rate)
- **‚è±Ô∏è Processing Time**: **8.5 hours total** (509.8 minutes)
- **üöÄ Processing Rate**: **8.5 documents/minute** sustained rate
- **üí∞ Cost**: **$0 processing cost** + ~$22/month storage

#### **‚ö° V3 Parallel Processing Pipeline Performance:**
- **Worker Configuration**: 8 parallel workers on Mac Pro M3 (14-core)
- **System Stability**: Ran flawlessly for 8+ hours without crashes
- **Memory Usage**: Stable ~4% usage (1.5GB) throughout run
- **Data Integrity**: **100% preserved** - no truncation, all content processed
- **Error Handling**: Excellent isolation - failed docs didn't stop processing

#### **üéØ What This Achievement Delivers:**
1. **31,487 Business Document Chunks**: Fully searchable in Pinecone vector database
2. **Complete 2022 Deal Archive**: All deal documents now queryable via chat interface
3. **Proven Scalable Pipeline**: Ready to process 2023, 2024 datasets
4. **Perfect Foundation**: Ready for enhanced LLM metadata when needed
5. **ROI Delivered**: Thousands of documents searchable for pennies

---

## üöÄ **MAJOR UI MIGRATION: CHAINLIT ‚Üí STREAMLIT BUSINESS DOCUMENT INTELLIGENCE**

### **üéâ Complete UI Platform Migration Completed**
**Date**: August 1, 2025  
**Status**: ‚úÖ **FULLY OPERATIONAL** - Professional Streamlit interface with superior user experience

#### **üéØ Migration Results:**
1. **‚úÖ Streamlit Business Intelligence Platform**: Modern, professional interface at http://localhost:8501
2. **‚úÖ Enhanced Search Interface**: Large editable text area with example query buttons
3. **‚úÖ Advanced Filtering**: Vendor, client, document type, and year filters in sidebar
4. **‚úÖ Professional Display**: Clean markdown rendering with source document details
5. **‚úÖ Database Integration**: 13,056+ business documents fully searchable

#### **üõ†Ô∏è Migration Implementation:**

**New Streamlit Architecture:**
- **Standalone Application**: `streamlit-ui/app_standalone.py` - completely independent
- **Hybrid Search Integration**: Direct PineconeDocumentClient with GPT-4.1-mini responses
- **Professional UI**: Clean business interface with proper styling and layout
- **Environment Management**: Proper .env loading with python-dotenv integration

**UI Improvements Over Chainlit:**
- **Large Text Area**: 150px height editable query input with multi-line support
- **Example Queries**: One-click buttons that populate the search field
- **Smart Filters**: Sidebar with vendor, client, document type, and year filtering
- **Source Transparency**: Expandable document cards with metadata and content previews
- **Search Statistics**: Real-time processing time, document count, and confidence metrics

#### **üéØ Technical Advantages:**
1. **No Authentication Complexity**: Direct access without Chainlit's authentication overhead
2. **Better Text Display**: Clean markdown rendering without formatting conflicts
3. **Professional Appearance**: Business-ready interface suitable for client presentations
4. **Faster Development**: Native Streamlit components vs. custom Chainlit workarounds
5. **Reliable Operation**: No hanging issues or display failures

#### **üîß New Technical Stack:**
- **Frontend**: Streamlit with custom CSS styling
- **Backend**: Direct PineconeDocumentClient + OpenAI GPT-4.1-mini integration
- **Search Engine**: Hybrid dense (multilingual-e5-large) + sparse (pinecone-sparse-english-v0) embeddings
- **Reranking**: Cohere rerank-3.5 for relevance optimization
- **Database**: 13,056 business documents in Pinecone "documents" namespace

#### **üìä Current Performance:**
- **Search Speed**: ~15-20 seconds for complex queries including LLM generation
- **Result Quality**: Professional business analysis with source citations
- **UI Responsiveness**: Immediate feedback with processing status indicators
- **Database Coverage**: 52 vendors, 50 clients, 6 years of business documents

---

## üö® **LOGGING SYSTEM ISSUE (COMPLETED)**

### **‚ùå Current Logging Problems Experienced:**
1. **‚ùå Impossible Monitoring**: Couldn't find active processing logs during 8-hour run
2. **‚ùå No Progress Visibility**: Had to guess completion status and progress
3. **‚ùå Results Archaeology**: Required JSON file analysis to get final statistics
4. **‚ùå Scattered Log Files**: 11 discovery logs + 5 production logs with unclear naming
5. **‚ùå No Real-time Feedback**: Process running in background with no ETA/progress

#### **üîç Evidence of Logging Chaos:**
```
logs/
‚îú‚îÄ‚îÄ discovery_20250726_101601.log      # Cryptic timestamps
‚îú‚îÄ‚îÄ discovery_20250726_101725.log      # Multiple discovery logs  
‚îú‚îÄ‚îÄ discovery_20250726_101819.log      # No clear naming
‚îú‚îÄ‚îÄ production_pipeline.log            # 16MB file - overwhelming
‚îú‚îÄ‚îÄ production_pipeline.log.1          # 104MB rotated logs
‚îú‚îÄ‚îÄ production_pipeline.log.2          # 104MB rotated logs
‚îî‚îÄ‚îÄ ...5 more rotated logs              # 520MB+ of logs!
```

#### **üéØ What Should Have Been Simple:**
- **During Run**: `tail -f current_processing.log` ‚Üí see "1,204/4,385 (27.5%) | ETA: 5h 23m"
- **After Completion**: `cat completion_summary.txt` ‚Üí instant results
- **Current Reality**: JSON archaeology + log file guessing games

---

## üéØ **IMMEDIATE PRIORITY: LOGGING SYSTEM OVERHAUL**

### **‚úÖ Solution Designed: Improved Logging Architecture**
**Document**: `IMPROVED_LOGGING_DESIGN.md` - Comprehensive logging redesign  
**Status**: Ready for implementation

#### **üéØ New Logging Structure (Designed):**
```
logs/
‚îú‚îÄ‚îÄ progress/
‚îÇ   ‚îú‚îÄ‚îÄ 2022_documents_progress_20250726_173700.log    # Real-time progress
‚îÇ   ‚îú‚îÄ‚îÄ current_processing_status.json                # Live status
‚îÇ   ‚îî‚îÄ‚îÄ latest_completion_summary.txt                 # Instant results
‚îú‚îÄ‚îÄ processing/
‚îÇ   ‚îú‚îÄ‚îÄ 2022_documents_20250726_173700_ACTIVE.log     # Processing details
‚îÇ   ‚îî‚îÄ‚îÄ 2022_documents_20250726_173700_SUMMARY.txt    # Final summary
‚îî‚îÄ‚îÄ system/
    ‚îú‚îÄ‚îÄ errors_20250726.log                           # Daily errors
    ‚îî‚îÄ‚îÄ performance_20250726.log                      # System metrics
```

#### **üöÄ Implementation Plan (Ready to Execute):**
1. **Phase 1**: `ProcessingProgressLogger` class with real-time updates
2. **Phase 2**: Structured logging with clear file naming
3. **Phase 3**: Easy monitoring commands (`tail -f progress/*.log`)

#### **üìà Success Criteria (Defined):**
- ‚úÖ **One command** to monitor any processing run
- ‚úÖ **Instant results** after completion without JSON archaeology  
- ‚úÖ **Clear progress** with ETA calculations
- ‚úÖ **Separate concerns** - discovery ‚â† processing ‚â† progress

---

## üéØ **Current Status Summary**

### **‚úÖ MAJOR ACHIEVEMENTS COMPLETED:**
1. **V3 Enhanced Architecture**: Production-ready with structured outputs
2. **Parallel Processing Pipeline**: 8-worker system proven at scale
3. **2022 Document Processing**: 4,319 documents successfully processed
4. **Pinecone Integration**: 31,487 chunks uploaded and searchable
5. **Cost Optimization**: $0 processing cost with smart design
6. **Data Integrity**: 100% content preservation (no truncation)

### **üéØ READY FOR PRODUCTION USE:**
1. **‚úÖ STREAMLIT BUSINESS INTELLIGENCE**: Professional document search and analysis platform
2. **‚úÖ HYBRID SEARCH ENGINE**: Advanced AI-powered document retrieval  
3. **‚úÖ SMART FILTERING**: Business metadata filters for precise document targeting
4. **‚úÖ BUSINESS DOCUMENT SEARCH**: 13,056+ documents from multiple years fully searchable

### **üí° Current Operational Status:**
- **‚úÖ STREAMLIT PLATFORM**: Running at http://localhost:8501 with full functionality
- **‚úÖ SEARCH INTERFACE**: Large text area with example queries and smart filtering
- **‚úÖ AI INTEGRATION**: GPT-4.1-mini generating professional business analysis
- **‚úÖ SOURCE TRANSPARENCY**: Expandable document cards with metadata and content previews
- **‚úÖ USER EXPERIENCE**: Clean, professional interface suitable for business presentations

---

## üîß **Technical Context (Preserved from Previous)**

### **Local Filesystem Migration** üéâ **100% COMPLETE - PRODUCTION READY**
- **Status**: Fully implemented, tested, and operational
- **Architecture**: Complete dual source support (Dropbox + Local) with abstract interface
- **Discovery Separation**: Standalone discovery with persistent JSON storage
- **Processing Separation**: Standalone processing from discovery results
- **Enhanced Progress Reporting**: Real-time progress, rate tracking, comprehensive logging
- **Business Metadata**: Perfect extraction with 1.1/1.1 confidence scores
- **LLM Classification**: GPT-4.1-mini working with 100% success rate

### **Streamlit Business Intelligence Platform** ‚úÖ **100% COMPLETE - FULLY OPERATIONAL**
- **Status**: Professional business document search and analysis at http://localhost:8501
- **Integration**: Direct PineconeDocumentClient + OpenAI GPT-4.1-mini
- **Features**: Advanced filtering, source citations, professional business analysis
- **Performance**: 15-20 second response times with comprehensive document analysis

### **V3 Enhanced Architecture** ‚úÖ **PRODUCTION PROVEN**
- **Enhanced Metadata**: Rich business intelligence extraction capabilities
- **Batch Processing**: 50% cost savings with OpenAI Batch API
- **Structured Outputs**: GPT-4.1-mini structured JSON responses
- **Search Enhancement**: Advanced filtering by pricing depth, vendor products, terms

### **Infrastructure Status**
- **Pinecone**: business-documents index with 60,490+ total records
- **Processing Power**: Mac Pro M3 (14-core) with excellent utilization potential
- **APIs**: OpenAI + Pinecone + Dropbox all operational
- **Parallel Processing**: 8-worker system validated for 8+ hour runs

## üéØ **Session Handoff for Logging Implementation**

**Ready to Implement:**
1. **ProcessingProgressLogger Class**: Real-time progress with ETA calculations
2. **Structured Log Files**: Clear naming and organization
3. **Monitoring Commands**: Simple `tail -f` and `cat` operations
4. **Progress Reporting**: "X/Y documents (Z%) | Rate: A docs/min | ETA: B"
5. **Completion Summaries**: Instant results without JSON archaeology

**Test Dataset Available:**
- 2023 discovery file ready for testing improved logging
- Can validate with smaller batches before large runs
- Mac Pro M3 ready for higher worker counts (12-16 workers)

**Expected Outcome:**
Transform from current logging chaos to **dead simple monitoring** where any processing run can be tracked with a single command and results are instantly available.

---

## üéØ **CURRENT STATUS: ENHANCED SALESFORCE DIRECT PROCESSING READY**

### **‚úÖ LATEST ACHIEVEMENTS DELIVERED (August 21, 2025)**
1. **Enhanced Salesforce Integration**: 85,404 documents with 27 optimized metadata fields ready for direct processing
2. **Production Pipeline Integration**: 90%+ code reuse with existing DocumentProcessor infrastructure
3. **Advanced Logging & Resume**: ProcessingProgressLogger with real-time ETA and automatic resume capability
4. **Performance Validation**: 104 docs/min stable processing rate with 100% success rate
5. **Business Intelligence Preservation**: All deal metadata, financial data, and content summaries maintained

### **üöÄ IMMEDIATE NEXT ACTIONS**
**Phase 1 Processing Ready**: Execute high-value document processing (64,123 docs - 10.3 hours)
```bash
python process_enhanced_salesforce_direct.py \
    --file-types .pdf .docx .msg \
    --namespace salesforce-enhanced-2025
```

**Monitoring Commands Available**:
- **Real-time Progress**: `tail -f logs/progress/*_progress_*.log`
- **Instant Results**: `cat logs/progress/latest_completion_summary.txt`
- **Resume Processing**: Add `--resume` flag to continue from any interruption
### **üìä PRODUCTION READINESS CONFIRMED**
- **Architecture**: Direct processing eliminates discovery file overhead
- **Performance**: Stable 104 docs/min with rich content extraction
- **Reliability**: Perfect resume capability with state saving every 100 documents
- **Business Value**: Complete deal intelligence with financial metrics and content summaries 
